# xpath
  ### 步骤
    需要安装lxml库
    导入lxml.etree
    etree.parse() —— 解析本地文件
      html_tree = etree.parse('XX.html')
    etree.HTML() —— 解析服务器响应的文件
      html_tree = etree.HTML(response.read().decode('utf-8'))
    html_tree.xpath(xpath路径)
  ### 基本语法
    1.路径查询
      //：查询所有子孙节点，不考虑层级关系
      /：找直接子节点
    2.谓词查询
      //div[@id]：查找div包含id内容的
      //div[@id='maincontent']：查找id='maincontent'
    3.属性查询
      //@class
    4.模糊查询
      //div[contains(@id, "he")]
      //div[starts-with(@id, "he")] 
    5.内容查询
      //div/h1/text() 获取h1标签里的内容
    6.逻辑运算
      //div[@id="head" and @class="s_down"]
      //title | //price
 # jsonpath
  ### 使用
    obj = json.load(open('json文件', 'r', encoding='utf-8))
    ret = jsonpath.jsonpath(obj, 'jsonpath语法')
    具体语法参考代码：jsonpath_basic_use.py
# BeautifulSoup
  ### 概念
    一个html解析器，主要功能也是解析和提取数据
    优缺点
      缺点：效率没有lxml高
      优点：接口设计人性化，使用方便
  ### 使用
    创建对象
      服务器响应的文件生成对象
        soup = BeautifulSoup(response.read().decode(), 'lxml')
      本地文件生成对象
        soup = BeautifulSoup(open('1.html'), 'lxml')
        注意：默认打开文件的编码格式为gbk
  ### 节点定位
    1.根据标签名查找节点
      soup.a.name——返回标签明
      soup.a.attrs——返回标签属性
    2.函数
      .find(返回一个对象)
        find('a') 只找到第一个a标签
        find('a', title='名字')
      .find_all(返回一个列表)
        find_all('a') 查找到所有的a
        find_all('a', 'span') 返回所有的a和span
        find_all('a', limit=2)  返回前两个a
      .select(根据选择器得到节点对象）
        1.element
        2. .class
        3. #id
        4.属性选择器
        5.层级选择器
    3.节点信息
      获取节点内容，适用于标签中嵌套标签的结构
        obj.string
        obj.get_text()
      节点的属性
        tag.name 获取标签名
        tag.attrs将属性值作为一个字典返回
      获取节点属性
        obj.attrs.get()
        obj.get()
        obj['']
